# ğŸ’³ Credit Fraud Detection - Production MLOps Pipeline

A production-ready MLOps pipeline for credit card fraud detection with automated testing, monitoring, and zero-downtime deployments.

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![MLflow](https://img.shields.io/badge/MLflow-2.16+-0194E2?style=flat&logo=mlflow&logoColor=white)](https://mlflow.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-336791?style=flat&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Prometheus](https://img.shields.io/badge/Prometheus-Monitoring-E6522C?style=flat&logo=prometheus&logoColor=white)](https://prometheus.io/)
[![Grafana](https://img.shields.io/badge/Grafana-Dashboards-F46800?style=flat&logo=grafana&logoColor=white)](https://grafana.com/)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Quick Start](#-quick-start)
- [Training & Deployment Workflow](#-training--deployment-workflow)
- [Production Features](#-production-features)
- [API Endpoints](#-api-endpoints)
- [Monitoring & Observability](#-monitoring--observability)
- [Testing](#-testing)
- [Project Structure](#-project-structure)
- [Troubleshooting](#-troubleshooting)

---

## âœ¨ Features

### ğŸ¤– **Model Training & Registry**
- âœ… Scikit-learn pipeline with StandardScaler + LogisticRegression
- âœ… MLflow experiment tracking and model registry
- âœ… Modern alias-based promotion (`production`, `challenger`, `champion`)
- âœ… DVC pipeline for reproducible training
- âœ… Automated model versioning

### ğŸš€ **Production Deployment**
- âœ… FastAPI REST API serving predictions
- âœ… Zero-downtime model updates via hot-reload
- âœ… Docker Compose multi-service orchestration
- âœ… Health checks and graceful degradation
- âœ… Prometheus metrics export

### ğŸ“Š **Monitoring & Observability**
- âœ… PostgreSQL prediction database with full audit trail
- âœ… Real-time data drift detection with Evidently
- âœ… Production health monitoring dashboard
- âœ… Prometheus + Grafana stack
- âœ… Latency and fraud rate tracking

### ğŸ§ª **Testing & Validation**
- âœ… Automated pytest suite (API + model tests)
- âœ… Pre-deployment model validation
- âœ… Performance threshold checks (AUC > 0.90)
- âœ… CI/CD pipeline with GitHub Actions

### ğŸ”„ **MLOps Best Practices**
- âœ… Automated promotion workflows
- âœ… Model validation before deployment
- âœ… Prediction logging and audit trails
- âœ… Data drift monitoring
- âœ… Reproducible training pipelines

---

## ğŸ—ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| ğŸ **ML Framework** | ![scikit-learn](https://img.shields.io/badge/scikit--learn-1.5-F7931E?style=flat&logo=scikit-learn&logoColor=white) | Model training and inference |
| ğŸš€ **API** | ![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=flat&logo=fastapi&logoColor=white) | High-performance REST API |
| ğŸ“Š **Experiment Tracking** | ![MLflow](https://img.shields.io/badge/MLflow-2.16-0194E2?style=flat&logo=mlflow&logoColor=white) | Model registry and tracking |
| ğŸ—„ï¸ **Database** | ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-336791?style=flat&logo=postgresql&logoColor=white) | Prediction audit trail |
| ğŸ“ˆ **Metrics** | ![Prometheus](https://img.shields.io/badge/Prometheus-Latest-E6522C?style=flat&logo=prometheus&logoColor=white) | Metrics collection |
| ğŸ“‰ **Visualization** | ![Grafana](https://img.shields.io/badge/Grafana-Latest-F46800?style=flat&logo=grafana&logoColor=white) | Dashboards and alerts |
| ğŸ³ **Containerization** | ![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=flat&logo=docker&logoColor=white) | Service orchestration |
| ğŸ”¬ **Drift Detection** | ![Evidently](https://img.shields.io/badge/Evidently-0.4-FF6B6B?style=flat) | Data quality monitoring |
| ğŸ§ª **Testing** | ![pytest](https://img.shields.io/badge/pytest-8.3-0A9EDC?style=flat&logo=pytest&logoColor=white) | Automated testing |
| ğŸ“¦ **Pipeline** | ![DVC](https://img.shields.io/badge/DVC-3.56-945DD6?style=flat&logo=dvc&logoColor=white) | Data versioning |

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Prerequisites

- âœ… Docker Desktop installed and running
- âœ… Python 3.11+ with virtual environment
- âœ… Git

### 2ï¸âƒ£ Setup Virtual Environment

```powershell
# Create and activate virtual environment
python -m venv creditfraud
creditfraud\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start Docker Services

```powershell
# Build and start all services
docker-compose -f infra/docker-compose.yaml up --build -d

# Verify containers are running
docker ps
```

### 4ï¸âƒ£ Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| ğŸš€ **API (FastAPI)** | http://localhost:8000 | - |
| ğŸ“Š **MLflow UI** | http://localhost:5000 | - |
| ğŸ“ˆ **Prometheus** | http://localhost:9090 | - |
| ğŸ“‰ **Grafana** | http://localhost:3000 | admin/admin |
| ğŸ—„ï¸ **PostgreSQL** | localhost:5432 | mlops/mlops_password |
| ğŸ“– **API Docs** | http://localhost:8000/docs | - |

### 5ï¸âƒ£ Health Check

```powershell
# Check API health
Invoke-RestMethod -Uri "http://localhost:8000/health"
# Expected: {"status":"ok"}

# Check model info
Invoke-RestMethod -Uri "http://localhost:8000/model_info"
```

---

## ğŸ¯ Training & Deployment Workflow

### Step 1: Train a Model

```powershell
# Activate virtual environment
creditfraud\Scripts\activate

# Set MLflow tracking URI
$env:MLFLOW_TRACKING_URI="http://localhost:5000"

# Train the model
python -m src.ml.train
```

**Output:**
```
âœ… Training complete!
Run ID: abc123...
AUC: 0.9756
Model Path: models/latest.joblib
MLflow Model URI: runs:/abc123.../model

To promote this model to production:
  python scripts/promote_model.py --version <VERSION> --alias production --reload-app
```

### Step 2: Validate Model

```powershell
# Validate model version 5
python scripts/validate_model.py --version 5

# Validate and auto-promote if it passes
python scripts/validate_model.py --version 5 --auto-promote
```

**Validation Checks:**
- âœ… Model loads successfully
- âœ… AUC > 0.90 threshold
- âœ… Predictions in valid range [0, 1]
- âœ… No NaN predictions
- âœ… Performance vs current production model

### Step 3: List Available Versions

```powershell
python scripts/promote_model.py --list
```

**Output:**
```
Available versions for model 'credit-fraud':
--------------------------------------------------------------------------------
Version    Run ID                              Aliases              Status
--------------------------------------------------------------------------------
5          abc123...                           None                 READY
4          def456...                           None                 READY
3          ghi789...                           production           READY
```

### Step 4: Promote to Production

```powershell
# Promote version 5 with automatic app reload
python scripts/promote_model.py --version 5 --alias production --reload-app
```

**What happens:**
1. âœ… Sets "production" alias to version 5 in MLflow
2. âœ… Calls app's `/reload` endpoint
3. âœ… App loads new model without restart (zero-downtime)

### Step 5: Verify Deployment

```powershell
# Check which model is serving
Invoke-RestMethod -Uri "http://localhost:8000/model_info"
```

**Expected:**
```json
{
  "model": {
    "name": "credit-fraud",
    "alias": "production",
    "version": "5",
    "source": "alias"
  },
  "errors": {
    "alias": null,
    "stage": null
  }
}
```

### Step 6: Make Predictions

```powershell
Invoke-RestMethod -Uri "http://localhost:8000/predict" -Method Post -ContentType "application/json" -Body '{
  "V1": -1.35, "V2": -0.07, "V3": 2.53, "V4": 1.38,
  "V5": -0.34, "V6": 0.46, "V7": 0.24, "V8": 0.10,
  "V9": 0.36, "V10": 0.09, "V11": -0.55, "V12": -0.62,
  "V13": -0.99, "V14": -0.31, "V15": 1.47, "V16": -0.47,
  "V17": 0.21, "V18": 0.03, "V19": 0.40, "V20": 0.25,
  "V21": -0.02, "V22": 0.28, "V23": -0.11, "V24": 0.07,
  "V25": 0.13, "V26": -0.19, "V27": 0.13, "V28": -0.02,
  "Amount": 149.62
}'
```

**Response:**
```json
{
  "fraud_probability": 0.1663,
  "prediction": 0,
  "model_version": "5"
}
```

---

## ğŸ›ï¸ Production Features

### 1. Prediction Database

All predictions are automatically logged to PostgreSQL:

```powershell
# Check prediction count
docker exec -it postgres psql -U mlops -d predictions -c "SELECT COUNT(*) FROM predictions;"

# View recent predictions
docker exec -it postgres psql -U mlops -d predictions -c "
  SELECT timestamp, fraud_probability, prediction, model_version, latency_ms
  FROM predictions
  ORDER BY timestamp DESC
  LIMIT 5;
"

# Analyze by model version
docker exec -it postgres psql -U mlops -d predictions -c "
  SELECT model_version, COUNT(*) as count, AVG(fraud_probability) as avg_prob
  FROM predictions
  GROUP BY model_version;
"
```

**Database Schema:**
```sql
CREATE TABLE predictions (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP,
    features JSON,
    fraud_probability FLOAT,
    prediction INTEGER,
    model_version VARCHAR,
    model_name VARCHAR,
    latency_ms FLOAT
);
```

### 2. Production Monitoring

```powershell
# Monitor last 1 hour
python -m src.monitoring.production_monitor --hours 1

# Monitor last 24 hours
python -m src.monitoring.production_monitor --hours 24
```

**Output:**
```
ğŸ” Production Monitoring Report - Last 1 hours
============================================================
ğŸ“Š Total Predictions: 4
ğŸ¯ Fraud Rate: 0.00%
ğŸ“ˆ Average Fraud Probability: 0.1663
âš¡ Average Latency: 128.03 ms

ğŸ¤– Model Versions:
  Version 5: 3 predictions (75.0%)

ğŸ”¬ Running Drift Analysis...
âœ… Drift report saved to: reports/production_drift_20251116_112309.html
âš ï¸  ALERT: Data drift detected!
```

**Monitoring Features:**
- ğŸ“ˆ Prediction volume tracking
- ğŸ¯ Fraud rate trends
- âš¡ API latency analysis
- ğŸ¤– Model version distribution
- ğŸ”¬ Data drift detection (Evidently)
- ğŸ“Š HTML drift reports
- âš ï¸ Automated alerts

### 3. Model Validation

```powershell
python scripts/validate_model.py --version 5
```

**Output:**
```
======================================================================
ğŸ” MODEL VALIDATION - credit-fraud v5
======================================================================

[1/5] Checking model exists...
  âœ“ Model version 5 found
    Run ID: 077d401bfc6c4a7493912db9729d8c09
    Status: READY

[2/5] Loading test data...
  âœ“ Loaded 56962 test samples

[3/5] Loading model and computing metrics...
  âœ“ AUC Score: 0.9756
    Precision (fraud): 0.8523
    Recall (fraud): 0.7891
    F1-Score (fraud): 0.8193

[4/5] Running validation checks...
  âœ“ PASS: AUC above threshold (0.90)
  âœ“ PASS: Predictions in valid range
  âœ“ PASS: No NaN predictions

[5/5] Comparing with production model...
  Production AUC: 0.9745
  New Model AUC: 0.9756
  Improvement: +0.0011 (+0.11%)
  âœ“ PASS: Performance acceptable vs production

======================================================================
âœ… VALIDATION PASSED
======================================================================

Model credit-fraud version 5 is ready for promotion!
  â€¢ AUC: 0.9756
  â€¢ All validation checks passed
```

---

## ğŸ”Œ API Endpoints

### Health & Info

```powershell
# Health check
GET http://localhost:8000/health

# Model information
GET http://localhost:8000/model_info

# Prometheus metrics
GET http://localhost:8000/metrics
```

### Model Management

```powershell
# Reload model (after promotion)
POST http://localhost:8000/reload
```

### Predictions

```powershell
# Make prediction
POST http://localhost:8000/predict
Content-Type: application/json

{
  "V1": -1.35,
  "V2": -0.07,
  ...
  "V28": -0.02,
  "Amount": 149.62
}
```

**Interactive API Docs:**
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

---

## ğŸ“Š Monitoring & Observability

### Prometheus Metrics

Access: http://localhost:9090

**Available Metrics:**
```
# Request count by endpoint
http_requests_total{method="POST", endpoint="/predict", status="200"}

# Request latency histogram
http_request_duration_seconds{endpoint="/predict"}

# Request latency summary
http_request_duration_seconds_sum
http_request_duration_seconds_count
```

**Sample Queries:**
```promql
# Request rate
rate(http_requests_total[5m])

# 95th percentile latency
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Error rate
rate(http_requests_total{status=~"4..|5.."}[5m])
```

### Grafana Dashboards

Access: http://localhost:3000 (admin/admin)

**Setup:**
1. Add Prometheus data source: http://prometheus:9090
2. Create dashboard with panels:
   - Request rate per endpoint
   - P50/P95/P99 latency
   - Error rate
   - Prediction distribution
   - Model version usage

### Drift Detection

```powershell
# Generate drift report
python -m src.monitoring.production_monitor --hours 24
```

**Output:**
- HTML report in `reports/` folder
- Data quality metrics
- Feature drift analysis
- Distribution comparisons
- Alerts for significant drift

---

## ğŸ§ª Testing

### Run All Tests

```powershell
# Activate virtual environment
creditfraud\Scripts\activate

# Run all tests
pytest -v

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

### Test Categories

**1. API Tests** (`tests/test_api.py`)
- âœ… Health endpoint
- âœ… Model info endpoint
- âœ… Prometheus metrics
- âœ… Valid predictions
- âœ… Invalid input handling
- âœ… Model reload

**2. Model Tests** (`tests/test_model.py`)
- âœ… Model exists
- âœ… Performance threshold (AUC > 0.90)
- âœ… Prediction range validation
- âœ… Data quality checks

### CI/CD Pipeline

Tests run automatically via GitHub Actions on:
- Every push to `main` or `develop`
- Every pull request to `main`

**Pipeline Stages:**
1. ğŸ§ª Run pytest with coverage
2. ğŸ¨ Lint with black, flake8, isort
3. ğŸ³ Build Docker images
4. ğŸ“¢ Notify on status

---

## ğŸ“ Project Structure

```
CreditFraudDetection/
â”œâ”€â”€ ğŸ“ configs/
â”‚   â”œâ”€â”€ base.yaml              # Base configuration
â”‚   â””â”€â”€ training.yaml          # Training hyperparameters
â”œâ”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ processed/             # DVC-managed processed data
â”‚       â”œâ”€â”€ train.parquet
â”‚       â”œâ”€â”€ test.parquet
â”‚       â””â”€â”€ reference.parquet  # For drift detection
â”œâ”€â”€ ğŸ“ docker/
â”‚   â”œâ”€â”€ Dockerfile.app         # FastAPI app image
â”‚   â””â”€â”€ Dockerfile.mlflow      # MLflow server image
â”œâ”€â”€ ğŸ“ infra/
â”‚   â”œâ”€â”€ docker-compose.yaml    # Service orchestration
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml     # Prometheus config
â”œâ”€â”€ ğŸ“ mlflow/
â”‚   â””â”€â”€ mlflow.db              # MLflow metadata database
â”œâ”€â”€ ğŸ“ mlruns/                 # MLflow artifacts (shared volume)
â”œâ”€â”€ ğŸ“ models/
â”‚   â””â”€â”€ latest.joblib          # Local model backup
â”œâ”€â”€ ğŸ“ reports/                # Drift and monitoring reports
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ promote_model.py       # Promote + reload
â”‚   â”œâ”€â”€ promote_and_restart.py # Promote + restart
â”‚   â”œâ”€â”€ validate_model.py      # Model validation
â”‚   â””â”€â”€ README.md              # Promotion workflow docs
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api.py             # FastAPI endpoints
â”‚   â”‚   â””â”€â”€ metrics.py         # Prometheus metrics
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ models.py          # PostgreSQL models
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ data.py            # Data preparation
â”‚   â”‚   â”œâ”€â”€ train.py           # Model training
â”‚   â”‚   â””â”€â”€ evaluate.py        # Model evaluation
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ drift_job.py       # Evidently drift detection
â”‚       â””â”€â”€ production_monitor.py  # Production monitoring
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ test_api.py            # API endpoint tests
â”‚   â””â”€â”€ test_model.py          # Model validation tests
â”œâ”€â”€ ğŸ“ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yaml            # GitHub Actions CI/CD
â”œâ”€â”€ dvc.yaml                   # DVC pipeline definition
â”œâ”€â”€ dvc.lock                   # DVC pipeline lock file
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ³ Docker Commands

### Start/Stop Services

```powershell
# Start all services
docker-compose -f infra/docker-compose.yaml up -d

# Stop all services
docker-compose -f infra/docker-compose.yaml down

# Rebuild specific service
docker-compose -f infra/docker-compose.yaml build app
docker-compose -f infra/docker-compose.yaml up -d app

# Restart specific service
docker-compose -f infra/docker-compose.yaml restart app
docker-compose -f infra/docker-compose.yaml restart mlflow

# View logs
docker logs app
docker logs mlflow
docker logs -f app  # Follow logs
```

### Container Management

```powershell
# Check running containers
docker ps

# Execute command in container
docker exec -it app sh
docker exec -it mlflow sh

# Check resource usage
docker stats

# Clean up stopped containers
docker-compose -f infra/docker-compose.yaml down -v
```

---

## ğŸ› Troubleshooting

### Model Not Loading

**Check model info:**
```powershell
Invoke-RestMethod -Uri "http://localhost:8000/model_info"
```

If `"source": "local"`, the model failed to load from MLflow. Check:
1. MLflow is running: `docker ps | findstr mlflow`
2. Artifacts exist: `dir mlruns\<experiment-id>\<run-id>\artifacts\model`
3. Reload the app: `Invoke-RestMethod -Uri "http://localhost:8000/reload" -Method Post`

### Containers Won't Start

```powershell
# Check logs
docker logs mlflow
docker logs app

# Common fixes
docker-compose -f infra/docker-compose.yaml down
docker-compose -f infra/docker-compose.yaml up --build -d
```

### Database Connection Issues

```powershell
# Check PostgreSQL logs
docker logs postgres

# Restart PostgreSQL
docker-compose -f infra/docker-compose.yaml restart postgres

# Check if app can connect
docker logs app | findstr -i database
```

### Predictions Not Being Logged

1. Check app logs: `docker logs app`
2. Verify environment variable: `docker exec app printenv DATABASE_URL`
3. Check if table exists:
   ```powershell
   docker exec -it postgres psql -U mlops -d predictions -c "\dt"
   ```

### Tests Failing

```powershell
# Make sure virtual environment is activated
creditfraud\Scripts\activate

# Install test dependencies
pip install pytest pytest-cov

# Run tests with verbose output
pytest -vv
```

### Training Fails

```powershell
# Make sure virtual environment is activated
creditfraud\Scripts\activate

# Set MLflow URI
$env:MLFLOW_TRACKING_URI="http://localhost:5000"

# Train again
python -m src.ml.train
```

---

## ğŸ“š Key Learnings

### MLflow Modern Practices

âœ… **DO**: Use aliases (`production`, `challenger`, `champion`)
âŒ **DON'T**: Use deprecated stages (`Staging`, `Production`)

### Model Promotion Workflow

1. **Train** â†’ Creates versioned model
2. **Validate** â†’ Check performance thresholds
3. **Promote** â†’ Set alias to version
4. **Reload/Restart** â†’ App serves new model

### Zero-Downtime Deployment

Use `/reload` endpoint for instant model updates without container restart.

### Feature Engineering

- **Time** feature excluded from training (only for record-keeping)
- Features: V1-V28 (PCA-transformed) + Amount
- StandardScaler applied before LogisticRegression

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Make changes and test: `pytest -v`
4. Commit: `git commit -m 'Add amazing feature'`
5. Push: `git push origin feature/amazing-feature`
6. Submit pull request
